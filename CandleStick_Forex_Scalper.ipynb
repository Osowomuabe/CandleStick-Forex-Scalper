{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HQQc-Oru8LL",
        "outputId": "218b3b92-a971-43d5-c7d6-0b6b46b9c6b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files extracted to: ./extracted_data\n",
            "OHLC data loaded successfully.\n",
            "Candlestick images saved to: ./extracted_data/candlestick_images\n",
            "Loaded 116664 images and labels.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2917/2917\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m548s\u001b[0m 187ms/step - accuracy: 0.5124 - loss: 0.6934 - val_accuracy: 0.5131 - val_loss: 0.6929\n",
            "Epoch 2/10\n",
            "\u001b[1m2917/2917\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m568s\u001b[0m 189ms/step - accuracy: 0.5126 - loss: 0.6929 - val_accuracy: 0.5131 - val_loss: 0.6929\n",
            "Epoch 3/10\n",
            "\u001b[1m2917/2917\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m562s\u001b[0m 189ms/step - accuracy: 0.5121 - loss: 0.6929 - val_accuracy: 0.5131 - val_loss: 0.6931\n",
            "Epoch 4/10\n",
            "\u001b[1m2917/2917\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m565s\u001b[0m 190ms/step - accuracy: 0.5120 - loss: 0.6930 - val_accuracy: 0.5131 - val_loss: 0.6929\n",
            "Epoch 5/10\n",
            "\u001b[1m2917/2917\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m550s\u001b[0m 189ms/step - accuracy: 0.5081 - loss: 0.6930 - val_accuracy: 0.5131 - val_loss: 0.6928\n",
            "Epoch 6/10\n",
            "\u001b[1m2917/2917\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m552s\u001b[0m 189ms/step - accuracy: 0.5100 - loss: 0.6930 - val_accuracy: 0.5131 - val_loss: 0.6928\n",
            "Epoch 7/10\n",
            "\u001b[1m2917/2917\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m544s\u001b[0m 187ms/step - accuracy: 0.5072 - loss: 0.6931 - val_accuracy: 0.5131 - val_loss: 0.6928\n",
            "Epoch 8/10\n",
            "\u001b[1m2917/2917\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m552s\u001b[0m 189ms/step - accuracy: 0.5099 - loss: 0.6930 - val_accuracy: 0.5131 - val_loss: 0.6928\n",
            "Epoch 9/10\n",
            "\u001b[1m2917/2917\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m546s\u001b[0m 187ms/step - accuracy: 0.5115 - loss: 0.6929 - val_accuracy: 0.5131 - val_loss: 0.6928\n",
            "Epoch 10/10\n",
            "\u001b[1m2917/2917\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m551s\u001b[0m 189ms/step - accuracy: 0.5091 - loss: 0.6931 - val_accuracy: 0.5131 - val_loss: 0.6929\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved successfully.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Define constants\n",
        "ZIP_FILE = \"archive.zip\"  # Path to your zip file\n",
        "EXTRACTION_DIR = \"./extracted_data\"\n",
        "IMAGE_SIZE = (64, 64)  # Image dimensions (height, width)\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "\n",
        "# Step 1: Extract the ZIP file\n",
        "def extract_zip_file(zip_path, extraction_directory):\n",
        "    if not os.path.exists(extraction_directory):\n",
        "        os.makedirs(extraction_directory)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extraction_directory)\n",
        "    print(f\"Files extracted to: {extraction_directory}\")\n",
        "\n",
        "# Step 2: Load and preprocess the OHLC data\n",
        "def load_ohlc_data(extraction_dir):\n",
        "    \"\"\"Load OHLC data and preprocess it for candlestick image creation.\"\"\"\n",
        "    csv_files = glob.glob(os.path.join(extraction_dir, \"*.csv\"))\n",
        "    if not csv_files:\n",
        "        raise FileNotFoundError(\"No CSV file found in the extraction directory.\")\n",
        "\n",
        "    ohlc = pd.read_csv(csv_files[0])\n",
        "    # Ensure columns have correct formatting\n",
        "    ohlc = ohlc[['open', 'high', 'low', 'close']].dropna().reset_index(drop=True)\n",
        "    print(\"OHLC data loaded successfully.\")\n",
        "    return ohlc\n",
        "\n",
        "# Step 3: Create candlestick images from OHLC data\n",
        "def create_candlestick_images(ohlc, save_dir, image_size=(64, 64)):\n",
        "    \"\"\"Create images of 3-candle formations and save them to disk.\"\"\"\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    labels = []\n",
        "    for i in range(len(ohlc) - 3):\n",
        "        # Extract three consecutive candlesticks\n",
        "        data = ohlc.iloc[i:i+3]\n",
        "\n",
        "        # Create a blank canvas\n",
        "        canvas = np.zeros((image_size[0], image_size[1], 3), dtype=np.uint8)\n",
        "\n",
        "        # Scale OHLC prices to fit within the canvas\n",
        "        min_price = data[['open', 'high', 'low', 'close']].min().min()\n",
        "        max_price = data[['open', 'high', 'low', 'close']].max().max()\n",
        "        scale = image_size[1] / (max_price - min_price)\n",
        "\n",
        "        # Draw candlesticks\n",
        "        for j, row in enumerate(data.itertuples()):\n",
        "            x = int((j + 1) * (image_size[0] / 4))  # Position on the x-axis\n",
        "            open_price = int((row.open - min_price) * scale)\n",
        "            close_price = int((row.close - min_price) * scale)\n",
        "            high_price = int((row.high - min_price) * scale)\n",
        "            low_price = int((row.low - min_price) * scale)\n",
        "\n",
        "            color = (0, 255, 0) if row.close >= row.open else (255, 0, 0)  # Green for bullish, red for bearish\n",
        "            cv2.line(canvas, (x, image_size[1] - low_price), (x, image_size[1] - high_price), (255, 255, 255), 1)\n",
        "            cv2.rectangle(canvas, (x - 3, image_size[1] - open_price), (x + 3, image_size[1] - close_price), color, -1)\n",
        "\n",
        "        # Save image\n",
        "        image_path = os.path.join(save_dir, f\"candlestick_{i}.png\")\n",
        "        cv2.imwrite(image_path, canvas)\n",
        "\n",
        "        # Determine label (1 for bullish, 0 for bearish) based on the next candlestick\n",
        "        next_close = ohlc.iloc[i+3]['close']\n",
        "        current_close = ohlc.iloc[i+2]['close']\n",
        "        labels.append(1 if next_close > current_close else 0)\n",
        "\n",
        "    print(f\"Candlestick images saved to: {save_dir}\")\n",
        "    return labels\n",
        "\n",
        "# Step 4: Load and preprocess image data for CNN\n",
        "def load_image_data(image_dir, labels, image_size=(64, 64)):\n",
        "    \"\"\"Load image data and associate it with corresponding labels.\"\"\"\n",
        "    images = []\n",
        "    for image_path in sorted(glob.glob(os.path.join(image_dir, \"*.png\"))):\n",
        "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "        image = cv2.resize(image, image_size)\n",
        "        images.append(image)\n",
        "\n",
        "    images = np.array(images).reshape(-1, image_size[0], image_size[1], 1)  # Reshape for CNN\n",
        "    labels = to_categorical(labels, 2)  # Convert labels to one-hot encoding (binary classification)\n",
        "\n",
        "    # Normalize image pixel values\n",
        "    images = images / 255.0\n",
        "\n",
        "    print(f\"Loaded {len(images)} images and labels.\")\n",
        "    return images, labels\n",
        "\n",
        "# Step 5: Train the CNN model\n",
        "def train_image_model(X, y, epochs=10, batch_size=32):\n",
        "    \"\"\"Train a CNN model using the preprocessed image data.\"\"\"\n",
        "    # Split dataset\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Define CNN model\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(2, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Augment data with ImageDataGenerator\n",
        "    datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, height_shift_range=0.1, zoom_range=0.1)\n",
        "    datagen.fit(X_train)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(datagen.flow(X_train, y_train, batch_size=batch_size),\n",
        "                        validation_data=(X_test, y_test), epochs=epochs)\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    # Step 1: Extract ZIP file\n",
        "    extract_zip_file(ZIP_FILE, EXTRACTION_DIR)\n",
        "\n",
        "    # Step 2: Load and preprocess OHLC data\n",
        "    ohlc = load_ohlc_data(EXTRACTION_DIR)\n",
        "\n",
        "    # Step 3: Create candlestick images\n",
        "    image_dir = os.path.join(EXTRACTION_DIR, \"candlestick_images\")\n",
        "    labels = create_candlestick_images(ohlc, image_dir, IMAGE_SIZE)\n",
        "\n",
        "    # Step 4: Load image data\n",
        "    images, labels = load_image_data(image_dir, labels, IMAGE_SIZE)\n",
        "\n",
        "    # Step 5: Train image model\n",
        "    model, history = train_image_model(images, labels, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Save the trained model\n",
        "    model.save(\"candlestick_image_model.h5\")\n",
        "    print(\"Model saved successfully.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igvU44vnx6RW",
        "outputId": "cec6c8d1-e5f3-41a8-bb13-67e5122ec0f5"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files extracted to: ./extracted_data\n",
            "Loaded OHLC data successfully.\n",
            "Batch processed: 0-1000, Total images: 1000\n",
            "Batch processed: 1000-2000, Total images: 2000\n",
            "Batch processed: 2000-3000, Total images: 3000\n",
            "Batch processed: 3000-4000, Total images: 4000\n",
            "Batch processed: 4000-5000, Total images: 5000\n",
            "Batch processed: 5000-6000, Total images: 6000\n",
            "Batch processed: 6000-7000, Total images: 7000\n",
            "Batch processed: 7000-8000, Total images: 8000\n",
            "Batch processed: 8000-9000, Total images: 9000\n",
            "Batch processed: 9000-10000, Total images: 10000\n",
            "Batch processed: 10000-11000, Total images: 11000\n",
            "Batch processed: 11000-12000, Total images: 12000\n",
            "Batch processed: 12000-13000, Total images: 13000\n",
            "Batch processed: 13000-14000, Total images: 14000\n",
            "Batch processed: 14000-15000, Total images: 15000\n",
            "Batch processed: 15000-16000, Total images: 16000\n",
            "Batch processed: 16000-17000, Total images: 17000\n",
            "Batch processed: 17000-18000, Total images: 18000\n",
            "Batch processed: 18000-19000, Total images: 19000\n",
            "Batch processed: 19000-20000, Total images: 20000\n",
            "Batch processed: 20000-21000, Total images: 21000\n",
            "Batch processed: 21000-22000, Total images: 22000\n",
            "Batch processed: 22000-23000, Total images: 23000\n",
            "Batch processed: 23000-24000, Total images: 24000\n",
            "Batch processed: 24000-25000, Total images: 25000\n",
            "Batch processed: 25000-26000, Total images: 26000\n",
            "Batch processed: 26000-27000, Total images: 27000\n",
            "Batch processed: 27000-28000, Total images: 28000\n",
            "Batch processed: 28000-29000, Total images: 29000\n",
            "Batch processed: 29000-30000, Total images: 30000\n",
            "Batch processed: 30000-31000, Total images: 31000\n",
            "Batch processed: 31000-32000, Total images: 32000\n",
            "Batch processed: 32000-33000, Total images: 33000\n",
            "Batch processed: 33000-34000, Total images: 34000\n",
            "Batch processed: 34000-35000, Total images: 35000\n",
            "Batch processed: 35000-36000, Total images: 36000\n",
            "Batch processed: 36000-37000, Total images: 37000\n",
            "Batch processed: 37000-38000, Total images: 38000\n",
            "Batch processed: 38000-39000, Total images: 39000\n",
            "Batch processed: 39000-40000, Total images: 40000\n",
            "Batch processed: 40000-41000, Total images: 41000\n",
            "Batch processed: 41000-42000, Total images: 42000\n",
            "Batch processed: 42000-43000, Total images: 43000\n",
            "Batch processed: 43000-44000, Total images: 44000\n",
            "Batch processed: 44000-45000, Total images: 45000\n",
            "Batch processed: 45000-46000, Total images: 46000\n",
            "Batch processed: 46000-47000, Total images: 47000\n",
            "Batch processed: 47000-48000, Total images: 48000\n",
            "Batch processed: 48000-49000, Total images: 49000\n",
            "Batch processed: 49000-50000, Total images: 50000\n",
            "Batch processed: 50000-51000, Total images: 51000\n",
            "Batch processed: 51000-52000, Total images: 52000\n",
            "Batch processed: 52000-53000, Total images: 53000\n",
            "Batch processed: 53000-54000, Total images: 54000\n",
            "Batch processed: 54000-55000, Total images: 55000\n",
            "Batch processed: 55000-56000, Total images: 56000\n",
            "Batch processed: 56000-57000, Total images: 57000\n",
            "Batch processed: 57000-58000, Total images: 58000\n",
            "Batch processed: 58000-59000, Total images: 59000\n",
            "Batch processed: 59000-60000, Total images: 60000\n",
            "Batch processed: 60000-61000, Total images: 61000\n",
            "Batch processed: 61000-62000, Total images: 62000\n",
            "Batch processed: 62000-63000, Total images: 63000\n",
            "Batch processed: 63000-64000, Total images: 64000\n",
            "Batch processed: 64000-65000, Total images: 65000\n",
            "Batch processed: 65000-66000, Total images: 66000\n",
            "Batch processed: 66000-67000, Total images: 67000\n",
            "Batch processed: 67000-68000, Total images: 68000\n",
            "Batch processed: 68000-69000, Total images: 69000\n",
            "Batch processed: 69000-70000, Total images: 70000\n",
            "Batch processed: 70000-71000, Total images: 71000\n",
            "Batch processed: 71000-72000, Total images: 72000\n",
            "Batch processed: 72000-73000, Total images: 73000\n",
            "Batch processed: 73000-74000, Total images: 74000\n",
            "Batch processed: 74000-75000, Total images: 75000\n",
            "Batch processed: 75000-76000, Total images: 76000\n",
            "Batch processed: 76000-77000, Total images: 77000\n",
            "Batch processed: 77000-78000, Total images: 78000\n",
            "Batch processed: 78000-79000, Total images: 79000\n",
            "Batch processed: 79000-80000, Total images: 80000\n",
            "Batch processed: 80000-81000, Total images: 81000\n",
            "Batch processed: 81000-82000, Total images: 82000\n",
            "Batch processed: 82000-83000, Total images: 83000\n",
            "Batch processed: 83000-84000, Total images: 84000\n",
            "Batch processed: 84000-85000, Total images: 85000\n",
            "Batch processed: 85000-86000, Total images: 86000\n",
            "Batch processed: 86000-87000, Total images: 87000\n",
            "Batch processed: 87000-88000, Total images: 88000\n",
            "Batch processed: 88000-89000, Total images: 89000\n",
            "Batch processed: 89000-90000, Total images: 90000\n",
            "Batch processed: 90000-91000, Total images: 91000\n",
            "Batch processed: 91000-92000, Total images: 92000\n",
            "Batch processed: 92000-93000, Total images: 93000\n",
            "Batch processed: 93000-94000, Total images: 94000\n",
            "Batch processed: 94000-95000, Total images: 95000\n",
            "Batch processed: 95000-96000, Total images: 96000\n",
            "Batch processed: 96000-97000, Total images: 97000\n",
            "Batch processed: 97000-98000, Total images: 98000\n",
            "Batch processed: 98000-99000, Total images: 99000\n",
            "Batch processed: 99000-100000, Total images: 100000\n",
            "Total candlestick images created: 100000\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m446s\u001b[0m 177ms/step - accuracy: 0.5074 - loss: 0.6936 - val_accuracy: 0.5167 - val_loss: 0.6927\n",
            "Epoch 2/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 177ms/step - accuracy: 0.5111 - loss: 0.6930 - val_accuracy: 0.5166 - val_loss: 0.6927\n",
            "Epoch 3/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 177ms/step - accuracy: 0.5109 - loss: 0.6930 - val_accuracy: 0.5164 - val_loss: 0.6927\n",
            "Epoch 4/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m435s\u001b[0m 174ms/step - accuracy: 0.5110 - loss: 0.6930 - val_accuracy: 0.5164 - val_loss: 0.6927\n",
            "Epoch 5/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 177ms/step - accuracy: 0.5110 - loss: 0.6929 - val_accuracy: 0.5163 - val_loss: 0.6927\n",
            "Epoch 6/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 170ms/step - accuracy: 0.5110 - loss: 0.6929 - val_accuracy: 0.5163 - val_loss: 0.6927\n",
            "Epoch 7/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m444s\u001b[0m 177ms/step - accuracy: 0.5110 - loss: 0.6929 - val_accuracy: 0.5165 - val_loss: 0.6927\n",
            "Epoch 8/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 171ms/step - accuracy: 0.5110 - loss: 0.6929 - val_accuracy: 0.5166 - val_loss: 0.6927\n",
            "Epoch 9/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 177ms/step - accuracy: 0.5110 - loss: 0.6929 - val_accuracy: 0.5165 - val_loss: 0.6927\n",
            "Epoch 10/10\n",
            "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m428s\u001b[0m 171ms/step - accuracy: 0.5110 - loss: 0.6929 - val_accuracy: 0.5162 - val_loss: 0.6927\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model training completed!\n",
            "CNN model saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# ---- Constants ----\n",
        "ZIP_FILE = \"archive.zip\"  # Replace with your dataset zip file\n",
        "EXTRACTION_DIR = \"./extracted_data\"\n",
        "IMAGE_SIZE = (64, 64)  # Image dimensions\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "MAX_IMAGES = 100000  # Optional: Limit the number of generated images for large datasets\n",
        "\n",
        "\n",
        "# ---- Step 1: Extract ZIP File ----\n",
        "def extract_zip_file(zip_path, extraction_dir):\n",
        "    \"\"\"Extract a ZIP file to a specific directory.\"\"\"\n",
        "    if not os.path.exists(extraction_dir):\n",
        "        os.makedirs(extraction_dir)\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extraction_dir)\n",
        "    print(f\"Files extracted to: {extraction_dir}\")\n",
        "\n",
        "\n",
        "# ---- Step 2: Load and Preprocess OHLC Data ----\n",
        "def load_ohlc_data(folder_path):\n",
        "    \"\"\"Load OHLC data from extracted CSV files.\"\"\"\n",
        "    csv_file = glob.glob(os.path.join(folder_path, \"*.csv\"))[0]  # Grab the first CSV file\n",
        "    if not csv_file:\n",
        "        raise FileNotFoundError(\"No OHLC data file found in the directory!\")\n",
        "    ohlc = pd.read_csv(csv_file)\n",
        "    ohlc = ohlc[['open', 'high', 'low', 'close']].dropna().reset_index(drop=True)\n",
        "    print(\"Loaded OHLC data successfully.\")\n",
        "    return ohlc\n",
        "\n",
        "\n",
        "# ---- Step 3: Create Candlestick Images in Chunks ----\n",
        "def create_candlestick_images(ohlc, output_dir, image_size=(64, 64), batch_size=1000):\n",
        "    \"\"\"Convert candlestick formations into images and generate corresponding labels in batches.\"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    labels = []\n",
        "    image_count = 0\n",
        "\n",
        "    # Ensure we stop when there are less than 4 candlesticks left (for labels)\n",
        "    for batch_start in range(0, min(len(ohlc) - 3, MAX_IMAGES), batch_size):\n",
        "        batch_end = min(batch_start + batch_size, len(ohlc) - 3)  # Adjust for last batch\n",
        "        batch_labels = []\n",
        "\n",
        "        for i in range(batch_start, batch_end):\n",
        "            # Extract three consecutive candlesticks\n",
        "            three_candles = ohlc.iloc[i:i + 3]\n",
        "            next_close = ohlc.iloc[i + 3]['close']  # Compare next candlestick's close\n",
        "            last_close = ohlc.iloc[i + 2]['close']\n",
        "\n",
        "            # Determine the label\n",
        "            batch_labels.append(1 if next_close > last_close else 0)\n",
        "\n",
        "            # Create a candlestick image canvas\n",
        "            canvas = np.zeros((*image_size, 3), dtype=np.uint8)\n",
        "            min_price = three_candles.min().min()\n",
        "            max_price = three_candles.max().max()\n",
        "            scale = (image_size[1] / (max_price - min_price))\n",
        "\n",
        "            # Draw candlesticks\n",
        "            for j, row in enumerate(three_candles.itertuples()):\n",
        "                x = int((j + 1) * (image_size[0] / 4))\n",
        "                open_price = int((row.open - min_price) * scale)\n",
        "                close_price = int((row.close - min_price) * scale)\n",
        "                high_price = int((row.high - min_price) * scale)\n",
        "                low_price = int((row.low - min_price) * scale)\n",
        "\n",
        "                # Green if bullish, red if bearish\n",
        "                color = (0, 255, 0) if row.close >= row.open else (255, 0, 0)\n",
        "                cv2.line(canvas, (x, image_size[1] - low_price), (x, image_size[1] - high_price), (255, 255, 255), 1)\n",
        "                cv2.rectangle(canvas, (x - 3, image_size[1] - open_price), (x + 3, image_size[1] - close_price), color, -1)\n",
        "\n",
        "            # Save the image\n",
        "            image_path = os.path.join(output_dir, f\"candle_{image_count}.png\")\n",
        "            cv2.imwrite(image_path, canvas)\n",
        "            image_count += 1\n",
        "\n",
        "        labels.extend(batch_labels)\n",
        "        print(f\"Batch processed: {batch_start}-{batch_end}, Total images: {image_count}\")\n",
        "\n",
        "    print(f\"Total candlestick images created: {image_count}\")\n",
        "    return labels\n",
        "\n",
        "\n",
        "# ---- Step 4: Data Generator for Training ----\n",
        "def data_generator(image_dir, labels, image_size=(64, 64), batch_size=32):\n",
        "    \"\"\"Yield batches of images and labels without exceeding the labels list.\"\"\"\n",
        "    images = sorted(glob.glob(os.path.join(image_dir, \"*.png\")))\n",
        "    num_samples = len(labels)  # Match the number of labels explicitly\n",
        "\n",
        "    while True:\n",
        "        for start_idx in range(0, num_samples, batch_size):\n",
        "            end_idx = min(start_idx + batch_size, num_samples)\n",
        "            batch_images = []\n",
        "            batch_labels = []\n",
        "\n",
        "            for idx in range(start_idx, end_idx):\n",
        "                # Ensure idx does not exceed the number of images or labels\n",
        "                if idx < len(images) and idx < len(labels):\n",
        "                    image = cv2.imread(images[idx], cv2.IMREAD_GRAYSCALE)\n",
        "                    image = cv2.resize(image, image_size)\n",
        "                    batch_images.append(image)\n",
        "                    batch_labels.append(labels[idx])\n",
        "                else:\n",
        "                    # Handle cases where idx exceeds the length of images or labels\n",
        "                    break\n",
        "\n",
        "            if len(batch_images) == 0:\n",
        "                break  # Stop if no images are left\n",
        "\n",
        "            # Format batches for CNN\n",
        "            batch_images = np.array(batch_images).reshape(-1, image_size[0], image_size[1], 1) / 255.0\n",
        "            batch_labels = to_categorical(batch_labels, num_classes=2)\n",
        "\n",
        "            yield batch_images, batch_labels\n",
        "\n",
        "\n",
        "# ---- Step 5: Train CNN Model ----\n",
        "def train_image_model(image_dir, labels, image_size=(64, 64), epochs=10, batch_size=32):\n",
        "    \"\"\"Train the CNN model using a data generator.\"\"\"\n",
        "    # Split data into training and validation sets\n",
        "    num_samples = len(labels)\n",
        "    train_size = int(0.8 * num_samples)\n",
        "\n",
        "    train_generator = data_generator(image_dir, labels[:train_size], image_size, batch_size)\n",
        "    val_generator = data_generator(image_dir, labels[train_size:], image_size, batch_size)\n",
        "\n",
        "    train_steps = train_size // batch_size\n",
        "    val_steps = (num_samples - train_size) // batch_size\n",
        "\n",
        "    # Define the CNN model\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(2, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model using the generated data\n",
        "    model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=train_steps,\n",
        "        validation_data=val_generator,\n",
        "        validation_steps=val_steps,\n",
        "        epochs=epochs\n",
        "    )\n",
        "\n",
        "    print(\"Model training completed!\")\n",
        "    return model\n",
        "\n",
        "\n",
        "# ---- Step 6: Main Function ----\n",
        "def main():\n",
        "    # Step 1: Extract the zip file\n",
        "    extract_zip_file(ZIP_FILE, EXTRACTION_DIR)\n",
        "\n",
        "    # Step 2: Load OHLC data\n",
        "    ohlc = load_ohlc_data(EXTRACTION_DIR)\n",
        "\n",
        "    # Step 3: Generate candlestick images and labels\n",
        "    image_dir = os.path.join(EXTRACTION_DIR, \"candlestick_images\")\n",
        "    labels = create_candlestick_images(ohlc, image_dir, IMAGE_SIZE)\n",
        "\n",
        "    # Step 4: Train the CNN model\n",
        "    model = train_image_model(image_dir, labels, image_size=IMAGE_SIZE, epochs=EPOCHS, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Step 5: Save the model\n",
        "    model.save(\"candlestick_cnn_model.h5\")\n",
        "    print(\"CNN model saved successfully!\")\n",
        "\n",
        "\n",
        "# ---- Run the Code ----\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGg9ZVkvrF5u"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}